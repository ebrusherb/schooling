\documentclass{article}
%\documentclass[aps, twocolumn, tightenlines,amscd,amsmath,amssymb,verbatim]{revtex4}
\usepackage{latexsym}
\usepackage{amssymb,amsmath}
\usepackage{custom2}
\usepackage{graphicx} % for figures
\usepackage{epstopdf} % so can use EPS or PDF figures
%\usepackage{subfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage[pdftex]{hyperref}
\usepackage{lscape}
\captionsetup{justification=RaggedRight, singlelinecheck=false}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\Tr}{\text{Tr}}
\newcommand{\z}{\mathscr{Z}}
%\newtheorem{claim}{Claim}

\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{1.4in}
\addtolength{\textheight}{1.4in}
\addtolength{\topmargin}{-.5in}

\pagestyle{empty}

\begin{document}


\begin{center}
{\bf \LARGE{Information Gathering Strategies}}
\vspace{10pt}
\\ Eleanor Brush
\\ October 31, 2014
\end{center}

\tableofcontents

\section{Introduction}
Birds in flocks, like animals in many types of social groups, can use their peers to learn about the environment and update their opinions about where to move in that environment.  Previous work on starlings has measured the number of other birds an individuals pays attention to and shown that this number of ``neighbors" leads to a social network that is conducive to the whole flock reaching consensus.  It is unclear, however, why an individual would change its behavior, i.e. the number of neighbors it learns from, to achieve this outcome.  Individual birds should optimize how well they learn about the environment and should minimize the cognitive and time costs of learning.  I find the circumstances in which this individual level optimization can produce the observed group level optimization.


\section{Model }
\subsection{Learning dynamics}
In this model, an individual's opinion changes with a probability that depends on its social interactions and the strength of the environmental stimulus. Specifically, an individual's opinion $\sigma_i$ can be either $1$ or $-1$ and $\sigma_i$ switches with probability $w_i(\sigma_i)=\frac{1}{2}(-\sigma_i\sum_jA_{ij}\sigma_j )(1-\beta_i\sigma_i)$ where $A_{ij}$ indicates the strength of the connection from $j$ to $i$ and $\beta_i$ indicates the strength of the environmental stimulus perceived by $i$.  The stochastic dynamics of the opinions can be written down with a master equation \cite{Glauber:1963fk}.  The vector $\vec{v}(t)$ of expected values, $q_i(t)=\langle \sigma_i(t)\rangle$, satisfies the following differential equations: \cite{Glauber:1963fk}
\begin{equation*}
\frac{d\vec{v}}{dt}=M\vec{v}+\vec{\beta}^T(\vec{1}-\vec{v}).
\end{equation*}
We simplify this by writing $\frac{d\vec{v}}{dt}=A\vec{v}+\vec{\beta}^T\vec{1}$, where $A=M-B$ and $B$ is a diagonal matrix with the values of $\vec{\beta}$ along the diagonal.

These stochastic dynamics will lead to an equilibrium probability distribution over sets of opinions that agrees with the Boltzmann equilibrium distribution for a system with energy or Hamiltonian given by 
\begin{equation*}
E(\vec{v})=-\vec{v}^TA\vec{v}-\vec{\beta}^T\vec{v}
\end{equation*}  

\subsection{Individual-level performance }
The group is made up of $N$ individuals.  Each individual has a strategy that determines how many other individuals he pays attention to, $n_i=\sum_{j\neq i}{\bf I}(A_{ij}\neq 0)$.  For a given number of iterations, we distribute the individuals randomly in space.  Each individual then pays attention to its $n_i$ nearest neighbors.  This is meant to capture One individual is chosen to be a receiver; it and all those individuals within a radius $r$ of the receiver perceive an external signal of strength $\beta$.  We find the expected opinion of each individual $q_i$ after a period of time $T$.  We refer to each of these random iterations as a signaling event.

In the absence of an external signal, neither opinion is correct.  When an external signal is presented, some individuals will be able to perceive it ($\beta_i>0$) and some individuals will not ($\beta_i=0$).  In this case, having an opinion that matches the external signal ($\sigma_i=1$) is beneficial.  How beneficial it is depends on the kind of signal.  We consider two types of signals: predators and food.  If a predator is present, we assume that whichever individual is least aware of the signal will be predated.  We therefore find, over many signaling events, the probability that an individual will be eaten and an individual's fitness is given by the probability of surviving.  If there are resources available, we assume that fitness is proportional to the probability of being the first to reach the resources. 

\subsection{Group-level performance }
We consider two group-level properties that indicate how cohesive the group is.  The first property is $\mathscr{H}_2$ robustness. This is a measure of the robustness of the consensus state in which all birds have the same opinion to noise at equilibrium (see Appendix Sec. \ref{H2}). The second property is the correlation length of the flock. This is the distance at which the average correlations between birds changes from positive to negative, i.e. the distance over which birds' opinions tend to be positively correlated with each other (see Appendix Sec \ref{corr}).

\subsection{Optimization methods }
To understand what strategies we might expect to find, we are interested in identifying the optimal strategies.  We does this in two ways.  First, using the framework of adaptive dynamics in finite populations, we identify the evolutionarily stable strategy.  This framework assumes that there is a homogeneous population into which an mutant individual tries to invade.  If strategies change on a learning rather than an evolutionary timescale, or if there is just a lot of variation in the population, we might expect individuals to try to optimize their strategies in the context of a heterogeneous population.  This is our second method of optimization.

\section{Results }

\subsection{ESS }
\subsection{Learned optima }
\subsection{Relationship between group properties }

\section{Discussion}

\section{Figures }
\begin{figure}[ht]
\includegraphics[width=6.83in]{/Users/eleanorbrush/Desktop/ESSneighbors.pdf}
\caption{\label{ESS} The ESS number of neighbors is always higher if selection is due to predation than due to resources.  The ESS number of neighbors when selection is due to predation is a non-monotonic function of how public the signal is. The x-axis indicates the radius of the signal and the y-axis indicates the ESS number of neighbors. 
}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=6.83in]{/Users/eleanorbrush/Desktop/greedyoptneighbors.pdf}
\caption{\label{greedyopt} When the birds choose the optimal number of neighbors, given the strategies the rest of the flock are using, they settle on higher strategies when selection is due to predation than when selection is due to resources. Under selection due to predation, the $\mathscr{H}_2$ robustness and correlation length of the flock tend to decrease, whereas under selection due to resources, $\mathscr{H}_2$ robustness is relatively constant and correlation length tends to increase. The upper row shows results from implementing selection based to predation and the lower row shows results from implementing selection based on resources. In each panel, the x-axis represents the number of times the birds are allowed to choose optimal strategies. The first column shows one example of how the birds' strategies change over time. The second column shows, for many initial conditions, how the $\mathscr{H}_2$ robustness changes over time. The third column shows, for many initial conditions, how the correlation length changes over time.
}
\end{figure}

\section{Appendix}
\subsection{H2 Norm \label{H2}} 

\ Let $y$ represent the distance between the vector of opinions and consensus, i.e.
$y=QxQ^T,$
where $Q\in\R^{n-1\times n}$ is such that $Q\vec{1}=0$ , $QQ^T=I_{n-1}$, and $Q^TQ=I_n-\frac{1}{n}\vec{1}\times\vec{1}^T$.   
Then 
\begin{equation}
\dot{y}(t)=-\overline{L}y(t)+Q\xi(t) \notag
\end{equation}
where $\overline{L}=QLQ^T$.  As shown in \cite{Young:2010fk}, if $\Sigma_y(t)=\E[y(t)y(t)^T]$,  $\Sigma_y$ at equilibrium solves
\begin{equation} 0=-\overline{L}\Sigma_y-\Sigma_y\overline{L}^T+D. \label{h2norm}
\end{equation}
\begin{align*}
\mathscr{H}_2&=\sqrt{\Tr(\Sigma_y)}
\\\mathscr{H}_2 \text{ robustness} &=\frac{1}{\mathscr{H}_2}
\end{align*}

\subsection{Calculating correlations \label{corr}}
The following is derived from the work in \cite{Bialek:2013fk}.
\begin{align*}
H(\vec{v})&=-\vec{v}^TA\vec{v}-\vec{\beta}^T\vec{v}
%\\&=\vec{v}A\vec{v}-\vec{\beta}^T\vec{v}+\vec{\beta}^T\vec{1}
\end{align*}
Define $V=\sum_kv_k/N$ and $\vec{y}=(\vec{v}-V\vec{1})/V$.  Then $\vec{v}=V(\vec{1}+\vec{y})$.  
\begin{align*}
H(\vec{y},V)&=-V^2(\vec{1}^T+\vec{y}^T)A(\vec{1}+\vec{y})-\vec{\beta}^TV(\vec{1}+\vec{y})
\\&=-V^2(\vec{1}^T+\vec{y}^T)A\vec{y}-V\vec{\beta}^T\vec{y}-V\vec{\beta}^T\vec{1} \text{ since $A\vec{1}=\vec{0}$}
\\&=-V^2\vec{y}^TA\vec{y}+(-V^2\vec{1}^TA-V\vec{\beta}^T)\vec{y}-V\vec{\beta}^T\vec{1}
\end{align*}
Define $Q\in M_{N-1,N}$ that rotates an $N$-vector away from the consensus vector and define $\vec{z}=Q\vec{y}$ so that $\vec{y}=Q^T\vec{z}$ since $\sum_ky_k=0$.
\begin{align*}
H(\vec{z},V)&=-V^2\vec{z}^TQAQ^T\vec{z}+(-V^2\vec{1}^TAQ^T-V\vec{\beta}^TQ^T)\vec{z}-V\vec{\beta}^T\vec{1}
\\&=V^2\vec{z}^TP\vec{z}+(-V^2\vec{\sigma}_1^T-V\vec{\sigma}_2^T)\vec{z}-V\vec{\beta}^T\vec{1}
\\ \Rightarrow P(\vec{z},V)&=\frac{1}{\z}\exp\left(-V^2\vec{z}^TP\vec{z}+(V^2\vec{\sigma}_1^T+V\vec{\sigma}_2^T)\vec{z}+V\vec{\beta}^T\vec{1}\right)
\end{align*}
where $P=-QAQ^T$, $\vec{\sigma}_1=QA^T\vec{1}$, and $\vec{\sigma}_2=Q\vec{\beta}$.


\begin{fact}
\begin{align*}
\int\exp\left(-\frac{1}{2}\vec{z}^TP\vec{z}+\vec{\sigma}^T\vec{z}\right)d^{N-1}z&=\sqrt{\frac{(2\pi)^{N-1}}{\det P}}\exp\left(-\frac{1}{2}\vec{\sigma}^TP^{-1}\vec{\sigma}\right)
\\\Rightarrow \int\exp\left(-\vec{z}^TP\vec{z}+\vec{\sigma}^T\vec{z}\right)d^{N-1}z&=\sqrt{\frac{(2\pi)^{N-1}}{2^{N-1}\det P}}\exp\left(-\frac{1}{4}\vec{\sigma}^TP^{-1}\vec{\sigma}\right)
\\&=\sqrt{\pi^{N-1}}\sqrt{\frac{1}{\det P}}\exp\left(-\frac{1}{4}\vec{\sigma}^TP^{-1}\vec{\sigma}\right)
\end{align*}
\end{fact}

\begin{align*}
\z&=\int_{-\infty}^\infty\int_{\R^{N-1}}\exp\left(V^2\vec{z}^TP\vec{z}+(V^2\vec{\sigma}_1^T+V\vec{\sigma}_2^T)\vec{z}+V\vec{\beta}^T\vec{1}\right)d^NzdV
\\&=\int_{-\infty}^\infty \exp(V\vec{\beta}^T\vec{1})\int_{\R^{N-1}}\exp\left(V^2\vec{z}^TP\vec{z}+(V^2\vec{\sigma}_1^T+V\vec{\sigma}_2^T)\vec{z}\right)d^NzdV
\\&=\sqrt{\pi^{N-1}}\int_{-\infty}^\infty\exp(V\vec{\beta}^T\vec{1})\times\left(\sqrt{\frac{1}{V^{2(N-1)}\det{P}}}\right)\exp\left(-\frac{1}{4}\vec{\sigma}_1^TP^{-1}\vec{\sigma}_1\right)\exp\left(-\frac{1}{4V}\vec{\sigma}_2^TP^{-1}\vec{\sigma}_2\right)dV
\\&=\sqrt{\pi^{N-1}}\sqrt{\frac{1}{\det PP}}\exp\left(-\frac{1}{4}\vec{\sigma}_1^TP^{-1}\vec{\sigma}_1\right)\int_{-\infty}^\infty\exp(V\vec{\beta}^T\vec{1})\times\left(\sqrt{\frac{1}{V^{2(N-1)}}}\right)\exp\left(-\frac{1}{4V}\vec{\sigma}_2^TP^{-1}\vec{\sigma}_2\right)dV
\\&=\sqrt{\pi^{N-1}}\sqrt{\frac{1}{\det P}}\exp\left(-\frac{1}{4}\vec{\sigma}_1^TP^{-1}\vec{\sigma}_1\right)\int_{-\infty}^\infty\frac{1}{V^{(N-1)}}\exp\left(V\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TP^{-1}\vec{\sigma}_2\right)dV
\\&=\sqrt{\pi^{N-1}}\sqrt{\frac{1}{\Pi_a\lambda_a}}\exp\left(-\frac{1}{4}\vec{\sigma}_1^TP^{-1}\vec{\sigma}_1\right)\int_{-\infty}^\infty\frac{1}{V^{(N-1)}}\exp\left(V\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TP^{-1}\vec{\sigma}_2\right)dV 
\\&\text{ where $\lambda_a$ are the eigenvalues of $P$}
\\ \Rightarrow -\log(\z)&=\text{constants}+\frac{1}{2}\sum_a\log(\lambda_a)+\frac{1}{4}\vec{\sigma}_1^TP^{-1}\vec{\sigma_1}-\log(f(P))
\end{align*}

where $f(P)=\int_{-\infty}^\infty\frac{1}{V^{(N-1)}}\exp\left(V\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TP^{-1}\vec{\sigma}_2\right)dV $.

\begin{align*}
\Rightarrow\frac{\partial -\log(\z)}{\partial P_{ij}}&=\frac{1}{2}\sum_a\frac{w_iw_j}{\lambda_a}+\frac{1}{4}\sum_{kl}\sigma_{1k}P^{-1}_{ki}P^{-1}_{jl}\sigma_{1l}-\frac{\partial \log f}{\partial P_{ij}}
\\\Rightarrow\frac{\partial -\log(\z)}{\partial P_{ij}}&=\frac{1}{2}\sum_a\frac{w_iw_j}{\lambda_a}+\frac{1}{4}\sum_{kl}\sigma_{1k}P^{-1}{ki}P^{-1}_{jl}\sigma_{1l}-\frac{1}{f}\frac{\partial f}{\partial P_{ij}}
\end{align*}

\begin{align*}
\frac{\partial f}{\partial P_{ij}}&=\int_{-\infty}^\infty\frac{1}{V^{(N-1)}}\exp\left(V\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TP^{-1}\vec{\sigma}_2\right)\left(-\frac{1}{4V}\sum_{kl}\sigma_{2k}P^{-1}_{ki}P^{-1}_{jl}\sigma_{2l}\right)dV
\\&=\left(-\frac{1}{4}\sum_{kl}\sigma_{2k}P^{-1}_{ki}P^{-1}_{jl}\sigma_{2l}\right)\int_{-\infty}^\infty\frac{1}{V^{(N)}}\exp\left(V\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TP^{-1}\vec{\sigma}_2\right)dV
\end{align*}

\small{
\begin{align*}
\Rightarrow\langle z_iz_j\rangle &=\frac{1}{2}\sum_a\frac{w_iw_j}{\lambda_a}+\frac{1}{4}\sum_{kl}\sigma_{1k}P^{-1}{ki}P^{-1}_{jl}\sigma_{1l}+\left(\frac{1}{4}\sum_{kl}\sigma_{2k}P^{-1}_{ki}P^{-1}_{jl}\sigma_{2l}\right)\frac{\int_{-\infty}^\infty\frac{1}{V^{(N)}}\exp\left(V\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TP^{-1}\vec{\sigma}_2\right)dV}{\int_{-\infty}^\infty\frac{1}{V^{(N-1)}}\exp\left(V\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TP^{-1}\vec{\sigma}_2\right)dV}
\end{align*}}


\subsection{Equivalence of dynamics and Hamiltonian }
\begin{claim}
If $H(\vec{v})=-\vec{v}^TA\vec{v}-\vec{\beta}^T\vec{v}$ and $\frac{d\vec{v}}{dt}=A\vec{v}+\vec{\beta}$ then $\nabla H\cdot \frac{d\vec{v}}{dt}\leq 0$.
\end{claim}
\begin{pf}
\begin{align*}
H(\vec{v})&=-\sum_{ij}A_{ij}v_iv_j-\sum_i\beta_iv_i
\\\Rightarrow\frac{\partial H}{\partial v_i}&=-\sum_{j\neq i}A_{ij}v_j-\sum_{j\neq i}A_{ji}v_j-2A_{ii}v_i-\beta_i
\\&=-\sum_j(A_{ij}+A^T_{ij})v_j-\beta_i
\\ \Rightarrow \nabla H&=-(A+A^T)\vec{v}-\vec{\beta}
\\ \Rightarrow \nabla H\cdot \frac{d\vec{v}}{dt}&=-\vec{v}^T(A+A^T)A\vec{v}-\vec{v}^T(A+A^T)\vec{\beta}-\vec{\beta}^TA\vec{v}-\vec{\beta}^T\vec{\beta}
\\&=-\vec{v}^TA^2\vec{v}-\vec{v}^TA^TA\vec{v}-\vec{v}^TA\vec{\beta}-2\vec{v}^TA^T\vec{\beta}-\vec{\beta}^T\vec{\beta}
\end{align*}
If $A$ is negative semi-definite, then $A^2$ is positive semi-definite and $A^TA$ is positive semi-definite.  (Consider that $\vec{v}^TA^TA\vec{v}=(A\vec{v})^T(A\vec{v})$ so that $\vec{v}A^TA\vec{v}=0$ if $A\vec{v}=0$ and $\vec{v}^TA^TA\vec{v}<0$ otherwise.) Therefore, $-\vec{v}^TA^2\vec{v}\leq 0$ and $-\vec{v}^TA^TA\vec{v}\leq 0$. At equilibrium, $\vec{v}$ will be ``close" to $\vec{\beta}$ so that $$-\vec{v}^TA\vec{\beta}-2\vec{v}^TA^T\vec{\beta}\sim-\vec{\beta}^TA\vec{\beta}$$
\end{pf}

\subsection{Numerically evaluating dynamics }
\begin{claim}
Let $\Phi(t)$ be the fundamental matrix solution to the homogeneous differential equations $\frac{d\vec{x}}{dt}=A\vec{x}$ so that $\Phi(t)=V\Lambda(t)$ where the columns of $V$ are the eigenvectors of $A$ and $$\Lambda(t)=\text{diag}(e^{\lambda_1 t},\dots,e^{\lambda_n t})$$ where $\{\lambda_i\}$ are the eigenvalues of $A$.  (Note that if $\lambda_i=0$ then $e^{\lambda _i t}=1$ for all $t$.) Let $V^\dagger$ be the pseudoinverse of $V$.  Let $\vec{c}=V^\dagger\vec{X_0}$.  Let $$G(t)=\text{diag}(1/\lambda_1(1-e^{-\lambda_1t}),\dots,1/\lambda_n(1-e^{-\lambda_nt}))V^\dagger\vec{\beta}.$$
(If $\lambda_i=0$ then the corresponding entry of the first part of $G(t)$ will be given by $t$ rather than the form above.)  THEN if we let $x(t)=\Phi(t)G(t)+\Phi(t)\vec{c}$, $X(t)$ solves the inhomogeneous equations $\frac{dx}{dt}=A\vec{x}+\vec{\beta}$ with initial conditions $x(0)=\vec{X_0}$.
\end{claim}

\begin{pf}
It is clear that
\begin{align*}
\frac{d\Phi\vec{c}}{dt}&=A\Phi(t)\vec{c} \text{ and } \Phi(0)\vec{c}=\vec{X_0}.
\end{align*}
Now, 
\begin{align*}
\Phi(t)G(t)&=V
\left(\begin{array}{ccccc}
e^{\lambda_1 t}  &\dots &0
\\ \vdots & \ddots &\vdots
\\ 0 & \dots & e^{\lambda_nt}
 \end{array}\right)
 \left(\begin{array}{ccccc}
\frac{1}{\lambda_1}(1-e^{\lambda_1 t})  &\dots &0
\\ \vdots & \ddots &\vdots
\\ 0 & \dots &\frac{1}{\lambda_n} (1-e^{\lambda_nt})
 \end{array}\right)
 V^\dagger\vec{\beta}
 \\&=V
 \left(\begin{array}{ccccc}
\frac{1}{\lambda_1}(e^{\lambda_1 t}-1)  &\dots &0
\\ \vdots & \ddots &\vdots
\\ 0 & \dots &\frac{1}{\lambda_n} (e^{\lambda_nt}-1)
 \end{array}\right)
 V^\dagger\vec{\beta}
 \\ \Rightarrow \frac{d\Phi(t)G(t)}{dt}&=V
 \left(\begin{array}{ccccc}
e^{\lambda_1 t}  &\dots &0
\\ \vdots & \ddots &\vdots
\\ 0 & \dots &e^{\lambda_nt}
 \end{array}\right)
 \\&=V
 \left(\begin{array}{ccccc}
e^{\lambda_1 t}  &\dots &0
\\ \vdots & \ddots &\vdots
\\ 0 & \dots &e^{\lambda_nt}
 \end{array}\right)
 V^\dagger\vec{\beta} -VV^\dagger\vec{\beta}+\vec{\beta}
 \\&=V
 \left(\begin{array}{ccccc}
e^{\lambda_1 t}-1  &\dots &0
\\ \vdots & \ddots &\vdots
\\ 0 & \dots &e^{\lambda_nt}-1
 \end{array}\right)
 V^\dagger\vec{\beta} +\vec{\beta}
  \\&=V
 \left(\begin{array}{ccccc}
\lambda_1e^{\lambda_1t}\cdot \frac{1}{\lambda_1}(1-e^{-\lambda_1 t})  &\dots &0
\\ \vdots & \ddots &\vdots
\\ 0 & \dots &\lambda_ne^{\lambda_nt}\cdot\frac{1}{\lambda_n}(1-e^{-\lambda_nt})
 \end{array}\right)
 V^\dagger\vec{\beta} +\vec{\beta}
  \\&=AV\Lambda(t)
 \left(\begin{array}{ccccc}
 \frac{1}{\lambda_1}(1-e^{-\lambda_1 t})  &\dots &0
\\ \vdots & \ddots &\vdots
\\ 0 & \dots &\frac{1}{\lambda_n}(1-e^{-\lambda_nt})
 \end{array}\right)
 V^\dagger\vec{\beta} +\vec{\beta}
 \\&=AV\Lambda(t)G(t)+\vec{\beta}
 \\&=A\Phi(t)G(t)+\vec{\beta}
\end{align*}
Therefore $\frac{dx}{dt}=A\Phi(t)G(t)+A\Phi(t)\vec{c}+\vec{\beta}=Ax(t)+\vec{\beta}$ and since $G(0)=\vec{0}$, $x(0)=\vec{X_0}$.
\end{pf}

\nocite{*}
\bibliographystyle{plain}
\bibliography{info_evo}

\end{document}


