\documentclass{article}
%\documentclass[aps, twocolumn, tightenlines,amscd,amsmath,amssymb,verbatim]{revtex4}
\usepackage{latexsym}
\usepackage{amssymb,amsmath}
\usepackage{custom2}
\usepackage{graphicx} % for figures
\usepackage{epstopdf} % so can use EPS or PDF figures
%\usepackage{subfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage[pdftex]{hyperref}
\usepackage{lscape}
\captionsetup{justification=RaggedRight, singlelinecheck=false}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\Tr}{\text{Tr}}
\newcommand{\z}{\mathscr{Z}}
%\newtheorem{claim}{Claim}

\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{1.4in}
\addtolength{\textheight}{1.4in}
\addtolength{\topmargin}{-.5in}

\pagestyle{empty}

\begin{document}


\begin{center}
{\bf \LARGE{Information Gathering Strategies}}
\vspace{10pt}
\\ Eleanor Brush
\\ May 1, 2014
\end{center}

\tableofcontents

\section{Introduction}
Birds in flocks, like animals in many types of social groups, can use their peers to learn about the environment and update their opinions about where to move in that environment.  Previous work on starlings has measured the number of other birds an individuals pays attention to and shown that this number of ``neighbors" leads to a social network that is conducive to the whole flock reaching consensus.  It is unclear, however, why an individual would change its behavior, i.e. the number of neighbors it learns from, to achieve this outcome.  Individual birds should optimize how well they learn about the environment and should minimize the cognitive and time costs of learning.  I find the circumstances in which this individual level optimization can produce the observed group level optimization.

\section{Learning dynamics}
In this model, an individual's opinion changes with a probability that depends on its social interactions and the strength of the environmental stimulus. Specifically, an individual's opinion $\sigma_i$ can be either $1$ or $-1$ and $\sigma_i$ switches with probability $w_i(\sigma_i)=\frac{1}{2}(-\sigma_i\sum_jA_{ij}\sigma_j )(1-\beta_i\sigma_i)$ where $A_{ij}$ indicates the strength of the connection from $j$ to $i$ and $\beta_i$ indicates the strength of the environmental stimulus perceived by $i$.  The stochastic dynamics of the opinions can be written down with a master equation \cite{Glauber:1963fk}.  The vector $\vec{q}(t)$ of expected values, $q_i(t)=\langle \sigma_i(t)\rangle$, satisfies the following differential equations: \cite{Glauber:1963fk}
\begin{equation*}
\frac{d\vec{q}}{dt}=A\vec{q}+\vec{\beta}^T(\vec{1}-\vec{q}).
\end{equation*}
%Similarly, the matrix $R(t)$ of covariances, $R_{ij}(t)=\langle \sigma_i(t)\sigma_j\rangle$, satisfies the following differential equations:
%\begin{equation*}
%\frac{dR}{dt}=AR+RA^T
%\end{equation*}
These stochastic dynamics will lead to an equilibrium probability distribution over sets of opinions that agrees with the Boltzmann equilibrium distribution for a system with energy or Hamiltonian given by 
\begin{equation*}
E(\vec{q})=-\vec{q}^TA\vec{q}+\vec{\beta}^T(\vec{1}-\vec{q})
\end{equation*}  

\section{Individual-level measures of performance}
The group is made up of $N$ individuals.  Each individual has a strategy that determines how many other individuals he pays attention to, $n_i=\sum_{j\neq i}{\bf I}(A_{ij}\neq 0)$.  For a given number of iterations, we distribute the individuals randomly in space.  Each individual then pays attention to its $n_i$ nearest neighbors.  This is meant to capture One individual is chosen to be a receiver; it and all those individuals within a radius $r$ of the receiver perceive an external signal of strength $\beta$.  We find the expected opinion of each individual $q_i$ after a period of time $T$.  We refer to each of these random iterations as a signaling event.

In the absence of an external signal, neither opinion is correct.  When an external signal is presented, some individuals will be able to perceive it ($\beta_i>0$) and some individuals will not ($\beta_i=0$).  In this case, having an opinion that matches the external signal ($\sigma_i=1$) is beneficial.  How beneficial it is depends on the kind of signal.  We consider two types of signals: predators and food.  If a predator is present, we assume that whichever individual is least aware of the signal will be predated.  We therefore find, over many signaling events, the probability that an individual will be eaten and an individual's fitness is given by the probability of surviving.  If the signal indicates the presence of food, on the other hand, fitness may be proportional to how well an individual learns the external signal.  


\section{Optimization methods}
To understand what strategies we might expect to find, we are interested in identifying the optimal strategies.  We does this in two ways.  First, using the framework of adaptive dynamics in finite populations, we identify the evolutionarily stable strategy.  This framework assumes that there is a homogeneous population into which an mutant individual tries to invade.  If strategies change on a learning rather than an evolutionary timescale, or if there is just a lot of variation in the population, we might expect individuals to try to optimize their strategies in the context of a heterogeneous population.  This is our second method of optimization.

\section{ESS }
\subsection{Predation }

\subsection{Access to food }

\section{Learned optima }

\subsection{Predation }

\subsection{Access to food }
\section{Comparison to data}

\section{Discussion}

\section{Appendix}

\begin{align*}
H(\vec{v}))&=-\vec{v}^TA\vec{v}+\vec{\beta}^T(\vec{1}-\vec{v})
%\\&=\vec{v}A\vec{v}-\vec{\beta}^T\vec{v}+\vec{\beta}^T\vec{1}
\end{align*}
Define $V=\sum_kv_k/N$ and $\vec{y}=(\vec{v}-V\vec{1})/V$.  Then $\vec{v}=V(\vec{1}+\vec{y})$.  
\begin{align*}
H(\vec{y},V)&=-V^2(\vec{1}^T+\vec{y}^T)A(\vec{1}+\vec{y})+\vec{\beta}^T(\vec{1}-V(\vec{1}+\vec{y}))
\\&=-V^2(\vec{1}^T+\vec{y}^T)A\vec{y}-V\vec{\beta}^T\vec{y}+(1-V)\vec{\beta}^T\vec{1} \text{ since $A\vec{1}=\vec{0}$}
\\&=-V^2\vec{y}^TA\vec{y}+(-V^2\vec{1}^TA-V\vec{\beta}^T)\vec{y}+(1-V)\vec{\beta}^T\vec{1}
\end{align*}
Define $Q\in M_{N-1,N}$ that rotates an $N$-vector away from the consensus vector and define $\vec{z}=Q\vec{y}$ so that $\vec{y}=Q^T\vec{z}$ since $\sum_ky_k=0$.
\begin{align*}
H(\vec{z},V)&=-V^2\vec{z}^TQAQ^T\vec{z}+(-V^2\vec{1}^TAQ^T-V\vec{\beta}^TQ^T)\vec{z}+(1-V)\vec{\beta}^T\vec{1}
\\&=-V^2\vec{z}^TM\vec{z}+(-V^2\vec{\sigma}_1^T-V\vec{\sigma}_2^T)\vec{z}+(1-V)\vec{\beta}^T\vec{1}
\\ \Rightarrow P(\vec{z},V)&=\exp\left(V^2\vec{z}^TM\vec{z}+(V^2\vec{\sigma}_1^T+V\vec{\sigma}_2^T)\vec{z}+(V-1)\vec{\beta}^T\vec{1}\right)
\end{align*}
where $M=QAQ^T$, $\vec{\sigma}_1=QA^T\vec{1}$, and $\vec{\sigma}_2=Q\vec{\beta}$.


\begin{fact}
\begin{align*}
\int\exp\left(-\frac{1}{2}\vec{z}^TM\vec{z}+\vec{B}^T\vec{z}\right)d^{N-1}z&=\sqrt{\frac{(2\pi)^{N-1}}{\det M}}\exp\left(-\frac{1}{2}\vec{B}^TM^{-1}\vec{B}\right)
\\\Rightarrow \int\exp\left(-\vec{z}^TM\vec{z}+\vec{B}^T\vec{z}\right)d^{N-1}z&=\sqrt{\frac{(2\pi)^{N-1}}{2^{N-1}\det M}}\exp\left(-\frac{1}{4}\vec{B}^TM^{-1}\vec{B}\right)
\\&=\sqrt{\pi^{N-1}}\sqrt{\frac{1}{\det M}}\exp\left(-\frac{1}{4}\vec{B}^TM^{-1}\vec{B}\right)
\end{align*}
\end{fact}

\begin{align*}
\z&=\int_{-\infty}^\infty\int_{\R^{N-1}}\exp\left(V^2\vec{z}^TM\vec{z}+(V^2\vec{\sigma}_1^T+V\vec{\sigma}_2^T)\vec{z}+(V-1)\vec{\beta}^T\vec{1}\right)d^NzdV
\\&=\int_{-\infty}^\infty \exp((V-1)\vec{\beta}^T\vec{1})\int_{\R^{N-1}}\exp\left(V^2\vec{z}^TM\vec{z}+(V^2\vec{\sigma}_1^T+V\vec{\sigma}_2^T)\vec{z}\right)d^NzdV
\\&=\sqrt{\pi^{N-1}}\int_{-\infty}^\infty\exp((V-1)\vec{\beta}^T\vec{1})\times\left(\sqrt{\frac{1}{V^{2(N-1)}\det{M}}}\right)\exp\left(-\frac{1}{4}\vec{\sigma}_1^TM^{-1}\vec{\sigma}_1\right)\exp\left(-\frac{1}{4V}\vec{\sigma}_2^TM^{-1}\vec{\sigma}_2\right)dV
\\&=\sqrt{\pi^{N-1}}\sqrt{\frac{1}{\det M}}\exp\left(-\frac{1}{4}\vec{\sigma}_1^TM^{-1}\vec{\sigma}_1\right)\int_{-\infty}^\infty\exp((V-1)\vec{\beta}^T\vec{1})\times\left(\sqrt{\frac{1}{V^{2(N-1)}}}\right)\exp\left(-\frac{1}{4V}\vec{\sigma}_2^TM^{-1}\vec{\sigma}_2\right)dV
\\&=\sqrt{\pi^{N-1}}\sqrt{\frac{1}{\det M}}\exp\left(-\frac{1}{4}\vec{\sigma}_1^TM^{-1}\vec{\sigma}_1\right)\int_{-\infty}^\infty\frac{1}{V^{(N-1)}}\exp\left((V-1)\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TM^{-1}\vec{\sigma}_2\right)dV
\\&=\sqrt{\pi^{N-1}}\sqrt{\frac{1}{\Pi_a\lambda_a}}\exp\left(-\frac{1}{4}\vec{\sigma}_1^TM^{-1}\vec{\sigma}_1\right)\int_{-\infty}^\infty\frac{1}{V^{(N-1)}}\exp\left((V-1)\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TM^{-1}\vec{\sigma}_2\right)dV 
\\&\text{ where $\lambda_a$ are the eigenvalues of $M$}
\\ \Rightarrow -\log(\z)&=\text{constants}+\frac{1}{2}\sum_a\log(\lambda_a)+\frac{1}{4}\vec{\sigma}_1^TM^{-1}\vec{\sigma_1}-\log(f(M))
\end{align*}

where $f(M)=\int_{-\infty}^\infty\frac{1}{V^{(N-1)}}\exp\left((V-1)\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TM^{-1}\vec{\sigma}_2\right)dV $.

\begin{align*}
\Rightarrow\frac{\partial -\log(\z)}{\partial M_{ij}}&=\frac{1}{2}\sum_a\frac{w_iw_j}{\lambda_a}+\frac{1}{4}\sum_{kl}\sigma_{1k}M^{-1}_{ki}M^{-1}_{jl}\sigma_{1l}-\frac{\partial \log f}{\partial M_{ij}}
\\\Rightarrow\frac{\partial -\log(\z)}{\partial M_{ij}}&=\frac{1}{2}\sum_a\frac{w_iw_j}{\lambda_a}+\frac{1}{4}\sum_{kl}\sigma_{1k}M^{-1}{ki}M^{-1}_{jl}\sigma_{1l}-\frac{1}{f}\frac{\partial f}{\partial M_{ij}}
\end{align*}

\begin{align*}
\frac{\partial f}{\partial M_{ij}}&=\int_{-\infty}^\infty\frac{1}{V^{(N-1)}}\exp\left((V-1)\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TM^{-1}\vec{\sigma}_2\right)\left(-\frac{1}{4V}\sum_{kl}\sigma_{2k}M^{-1}_{ki}M^{-1}_{jl}\sigma_{2l}\right)dV
\\&=\left(-\frac{1}{4}\sum_{kl}\sigma_{2k}M^{-1}_{ki}M^{-1}_{jl}\sigma_{2l}\right)\int_{-\infty}^\infty\frac{1}{V^{(N)}}\exp\left((V-1)\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TM^{-1}\vec{\sigma}_2\right)dV
\end{align*}

\begin{align*}
\Rightarrow \langle z_iz_j\rangle &=\frac{1}{2}\sum_a\frac{w_iw_j}{\lambda_a}+\frac{1}{4}\sum_{kl}\sigma_{1k}M^{-1}{ki}M^{-1}_{jl}\sigma_{1l}+\left(\frac{1}{4}\sum_{kl}\sigma_{2k}M^{-1}_{ki}M^{-1}_{jl}\sigma_{2l}\right)\frac{\int_{-\infty}^\infty\frac{1}{V^{(N)}}\exp\left((V-1)\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TM^{-1}\vec{\sigma}_2\right)dV}{\int_{-\infty}^\infty\frac{1}{V^{(N-1)}}\exp\left((V-1)\vec{\beta}^T\vec{1}-\frac{1}{4V}\vec{\sigma}_2^TM^{-1}\vec{\sigma}_2\right)dV}
\end{align*}

\nocite{*}
\bibliographystyle{plain}
\bibliography{info_evo}

\end{document}


